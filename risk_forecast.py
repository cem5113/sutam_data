#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
risk_forecast.py
- Ã‡oklu pencere (3H, 8H, 1D, 1W, 1M) iÃ§in runtime-anchored gelecek tahmini
- EÄŸitimden Ã§Ä±kan modelleri (models/sutam_{3h,8h,1d,1w,1m}.joblib) kullanÄ±r.
- Ã–zellikler: calendar + (persist) priors + son gÃ¶zlemlerden yan deÄŸiÅŸkenler
- Girdi: geÃ§miÅŸ agregasyon dosyalarÄ± (sf_crime_grid_{3h,8h,1d,1w,1m}.parquet)
- Ã‡Ä±ktÄ±: CSV + JSON (varsayÄ±lan: forecasts/forecast_<freq>.{csv,json})

KullanÄ±m:
  python risk_forecast.py --freq auto --horizon 72h --geoid 94107 --topk 50
  python risk_forecast.py --freq 1W  --horizon 90d --topk 100
  python risk_forecast.py --freq auto --horizon 1M
"""

from __future__ import annotations
import argparse, json, os
from pathlib import Path
from datetime import datetime, timezone, timedelta

import numpy as np
import pandas as pd

try:
    import joblib
except Exception as e:
    raise SystemExit("joblib gerekli: pip install joblib") from e


# ------------ Sabitler & Yol HaritasÄ± ------------
HERE = Path(__file__).resolve().parent

MODEL_PATHS = {
    "3H": HERE / "models" / "sutam_3h.joblib",
    "8H": HERE / "models" / "sutam_8h.joblib",
    "1D": HERE / "models" / "sutam_1d.joblib",
    "1W": HERE / "models" / "sutam_1w.joblib",
    "1M": HERE / "models" / "sutam_1m.joblib",
}

AGG_PATHS = {
    "3H": HERE / "sf_crime_grid_3h.parquet",
    "8H": HERE / "sf_crime_grid_8h.parquet",
    "1D": HERE / "sf_crime_grid_1d.parquet",
    "1W": HERE / "sf_crime_grid_1w.parquet",
    "1M": HERE / "sf_crime_grid_1m.parquet",
}

# pandas uyarÄ±sÄ±nÄ± Ã¶nlemek iÃ§in kÃ¼Ã§Ã¼k harf (h/d) kullanalÄ±m
FREQ_TO_PANDAS = {
    "3H": "3h",
    "8H": "8h",
    "1D": "1d",
    "1W": "7d",   # runtime-anchored: 7 gÃ¼n ileri
    "1M": None,   # 30g ileri Ã¶zel
}


# ------------ YardÄ±mcÄ±lar ------------
def now_utc() -> pd.Timestamp:
    return pd.Timestamp(datetime.now(timezone.utc))


def pick_freq_auto(horizon_str: str) -> str:
    """
    AUTO seÃ§im â€” SADECE mevcut frekanslardan (3H, 8H, 1D, 1W, 1M) dÃ¶ner.
    - â‰¤72h â†’ 3H
    - â‰¤30d â†’ 1D
    - â‰¤90d â†’ 1W
    - >90d â†’ 1M (yaklaÅŸÄ±k 30g adÄ±mlarla)
    """
    s = horizon_str.lower().strip()
    if s.endswith("h"):
        hours = float(s[:-1])
    elif s.endswith("d"):
        hours = float(s[:-1]) * 24
    elif s.endswith("w"):
        hours = float(s[:-1]) * 24 * 7
    elif s.endswith("m"):
        hours = float(s[:-1]) * 24 * 30
    else:
        raise ValueError("horizon biÃ§imi: 24h, 72h, 14d, 8w, 1m ...")

    if hours <= 72:
        return "3H"
    if hours <= 24 * 30:
        return "1D"
    if hours <= 24 * 90:
        return "1W"
    return "1M"


def parse_horizon(h: str) -> timedelta:
    s = h.lower().strip()
    if s.endswith("h"):
        return timedelta(hours=float(s[:-1]))
    if s.endswith("d"):
        return timedelta(days=float(s[:-1]))
    if s.endswith("w"):
        return timedelta(weeks=float(s[:-1]))
    if s.endswith("m"):
        # 1M â‰ˆ 30 gÃ¼n
        return timedelta(days=float(s[:-1]) * 30)
    raise ValueError("horizon biÃ§imi: 24h, 10d, 8w, 1m ...")


def block_id_for_hour(h: int, freq: str) -> int:
    """8H iÃ§in 0,1,2 (00-08,08-16,16-24); 3H iÃ§in 0..7; diÄŸerleri -1."""
    if freq == "8H":
        return int(h // 8)
    if freq == "3H":
        return int(h // 3)
    return -1


def build_future_index(freq: str, horizon: timedelta, start_utc: pd.Timestamp | None = None) -> pd.DatetimeIndex:
    t0 = start_utc.tz_convert("UTC") if isinstance(start_utc, pd.Timestamp) else now_utc()
    if freq in ("3H", "8H", "1D", "1W"):
        pd_freq = FREQ_TO_PANDAS[freq]
        end = t0 + horizon
        # inclusive='left' â†’ t0 sonrasÄ± slotlarÄ± Ã¼ret
        rng = pd.date_range(t0.floor("s"), end.ceil("s"), freq=pd_freq, tz="UTC", inclusive="left")
        if len(rng) and rng[0] < t0:
            rng = rng[rng >= t0]
        return rng
    elif freq == "1M":
        # her adÄ±m â‰ˆ 30 gÃ¼n ileri
        steps = max(1, int(np.ceil(horizon / timedelta(days=30))))
        vals = [t0 + i * timedelta(days=30) for i in range(steps)]
        return pd.DatetimeIndex(pd.to_datetime(vals, utc=True))
    else:
        raise ValueError(f"Bilinmeyen freq: {freq}")


def add_calendar(df: pd.DataFrame, t_col: str, freq: str) -> pd.DataFrame:
    s = pd.to_datetime(df[t_col], utc=True)
    df["year"] = s.dt.year.astype("int16")
    df["month"] = s.dt.month.astype("int8")
    df["day_of_week"] = s.dt.dayofweek.astype("int8")
    if freq in ("3H", "8H", "1D"):
        df["hour_start"] = s.dt.hour.astype("int8")
    if freq in ("3H", "8H"):
        df["block_id"] = df["hour_start"].apply(lambda h: block_id_for_hour(int(h), freq)).astype("int8")
    return df


def last_known_snapshot(agg_path: Path, keys_keep: list[str]) -> pd.DataFrame:
    """Agregasyondan, her GEOID iÃ§in son satÄ±rÄ± al (persist Ã¶zellikler iÃ§in)."""
    if not agg_path.exists():
        raise FileNotFoundError(f"Girdi yok: {agg_path}")
    df = pd.read_parquet(agg_path)
    need = {"GEOID", "t0"} | set(keys_keep)
    miss = need - set(df.columns)
    if miss:
        raise SystemExit(f"{agg_path.name} eksik kolon(lar): {miss}")
    df = df.sort_values(["GEOID", "t0"]).groupby("GEOID", as_index=False).tail(1)
    return df[["GEOID"] + keys_keep].copy()


def prepare_features(freq: str, horizon: timedelta, geoid: str | None) -> tuple[pd.DataFrame, str]:
    """Gelecek pencereler iÃ§in X oluÅŸtur. Priors/yan deÄŸiÅŸkenleri son bilinen deÄŸerlerle doldurur."""
    agg_path = AGG_PATHS.get(freq)
    if agg_path is None or not agg_path.exists():
        raise SystemExit(f"GeÃ§miÅŸ agregasyon dosyasÄ± bulunamadÄ±: {freq} â†’ {agg_path}")

    # Son bilinen Ã¶zetler (persist edilecek kolonlar)
    base_numeric_maybe = [
        "crime_count",
        "prior_cnt_28d", "prior_p_28d",
        "prior_cnt_180d", "prior_p_180d",
        # yan deÄŸiÅŸken Ã¶rnekleri (varsa persist):
        "wx_tavg", "wx_prcp", "poi_total_count", "bus_stop_count", "train_stop_count", "population",
    ]

    # ðŸ”§ HIZLI ÅžEMA OKUMA: read_parquet(..., columns=[]).columns  (nrows desteklenmez!)
    try:
        present_cols = set(pd.read_parquet(agg_path, columns=[]).columns)
    except Exception:
        # bazÄ± okumalarda columns=[] boÅŸ dÃ¶nebilir; tam okuyup sadece kolon alalÄ±m
        present_cols = set(pd.read_parquet(agg_path).columns)

    keep_cols = [c for c in base_numeric_maybe if c in present_cols]
    if not keep_cols:
        # HiÃ§biri yoksa, en azÄ±ndan priors ve crime_count'Ä± bekleyelim; yoksa sadece calendar ile ilerleriz
        keep_cols = [c for c in ["crime_count", "prior_cnt_28d", "prior_p_28d", "prior_cnt_180d", "prior_p_180d"] if c in present_cols]

    snap = last_known_snapshot(agg_path, keys_keep=keep_cols)

    # Hedef GEOID listesi
    if geoid:
        geoids = [str(geoid)]
        snap = snap[snap["GEOID"].isin(geoids)]
        if snap.empty:
            raise SystemExit(f"GEOID son Ã¶zetlerde yok: {geoid}")
    else:
        geoids = snap["GEOID"].astype(str).unique().tolist()

    # Gelecek zaman dizisi
    future_t0 = build_future_index(freq, horizon)
    if len(future_t0) == 0:
        raise SystemExit("Horizon Ã§ok kÄ±sa gÃ¶rÃ¼nÃ¼yor; Ã¼retilecek ileri pencere yok.")

    grid = pd.MultiIndex.from_product([geoids, future_t0], names=["GEOID", "t0"]).to_frame(index=False)

    # Takvim
    grid = add_calendar(grid, "t0", freq)

    # Persist Ã¶zellikler (GEOID bazlÄ± sabitler)
    X = grid.merge(snap, on="GEOID", how="left")

    # Eksik numerikleri median ile doldur
    for c in [col for col in X.columns if pd.api.types.is_numeric_dtype(X[col])]:
        if X[c].isna().any():
            X[c] = X[c].fillna(X[c].median())

    # Model tarafÄ±nÄ±n beklediÄŸi tipler
    for c in ("day_of_week", "block_id"):
        if c in X.columns:
            X[c] = pd.to_numeric(X[c], errors="ignore")
            if not pd.api.types.is_integer_dtype(X[c]):
                X[c] = pd.to_numeric(X[c], errors="coerce").fillna(-1).astype("int8")

    return X, agg_path.name


def load_model(freq: str):
    p = MODEL_PATHS.get(freq)
    if p is None:
        raise SystemExit(f"Model yolu haritasÄ±nda olmayan frekans: {freq}")
    if not p.exists():
        raise SystemExit(f"Model yok: {p}")
    return joblib.load(p)


def score_and_rank(model, X: pd.DataFrame, topk: int | None, geoid: str | None) -> pd.DataFrame:
    # Modelin pipeline'Ä± (OneHot/Scaler vs.) olduÄŸunu varsayÄ±yoruz; GEOID/t0 Ã§Ä±kar
    feat = X.drop(columns=["GEOID", "t0"], errors="ignore")
    proba = model.predict_proba(feat)[:, 1].astype(np.float32)

    out = X[["GEOID", "t0"]].copy()
    out["prob"] = proba

    # Basit beÅŸli tier
    if len(out) >= 5:
        q = out["prob"].quantile([0.8, 0.6, 0.4, 0.2]).to_list()
    else:
        q = [0.8, 0.6, 0.4, 0.2]

    def tier(p):
        if p >= q[0]: return "Ã‡ok YÃ¼ksek"
        if p >= q[1]: return "YÃ¼ksek"
        if p >= q[2]: return "Orta"
        if p >= q[3]: return "DÃ¼ÅŸÃ¼k"
        return "Ã‡ok DÃ¼ÅŸÃ¼k"

    out["tier"] = out["prob"].apply(tier)

    if geoid:
        # tek geoid iÃ§in zaman sÄ±ralÄ±
        out = out.sort_values("t0", ascending=True)
    else:
        # ÅŸehir geneli top-k (aynÄ± t0â€™lar Ã¼zerinde)
        out = out.sort_values(["t0", "prob"], ascending=[True, False])
        if topk and topk > 0:
            out = out.groupby("t0", as_index=False).head(int(topk))

    return out.reset_index(drop=True)


def save_outputs(df: pd.DataFrame, freq: str, outdir: Path):
    outdir.mkdir(parents=True, exist_ok=True)
    csv_p = outdir / f"forecast_{freq.lower()}.csv"
    json_p = outdir / f"forecast_{freq.lower()}.json"
    df_out = df.copy()
    df_out["t0_iso"] = pd.to_datetime(df_out["t0"], utc=True).dt.strftime("%Y-%m-%d %H:%M:%SZ")
    df_out.to_csv(csv_p, index=False)
    with open(json_p, "w", encoding="utf-8") as f:
        json.dump(df_out.to_dict(orient="records"), f, ensure_ascii=False, indent=2)
    return csv_p, json_p


# ------------ CLI ------------
def parse_args():
    p = argparse.ArgumentParser(description="SUTAM Ã§oklu pencere ileri tahmin")
    p.add_argument("--freq", type=str, default="auto",
                   help="auto | 3H | 8H | 1D | 1W | 1M")
    p.add_argument("--horizon", type=str, default="72h",
                   help="Ã–rn: 24h, 72h, 14d, 90d, 1m")
    p.add_argument("--geoid", type=str, default=None,
                   help="Tek GEOID iÃ§in tahmin (boÅŸsa ÅŸehir geneli)")
    p.add_argument("--topk", type=int, default=50,
                   help="Åžehir geneli iÃ§in her pencerede en yÃ¼ksek K")
    p.add_argument("--outdir", type=Path, default=Path("forecasts"))
    return p.parse_args()


def main():
    args = parse_args()
    freq = args.freq.upper().strip()
    if freq == "AUTO":
        freq = pick_freq_auto(args.horizon)

    if freq not in MODEL_PATHS or freq not in AGG_PATHS:
        raise SystemExit(f"Desteklenmeyen freq: {freq} (izinli: {list(MODEL_PATHS.keys())})")

    horizon_td = parse_horizon(args.horizon)

    print(f"[INFO] freq={freq}  horizon={args.horizon}  geoid={args.geoid or 'ALL'}")

    # Ã–zellikleri hazÄ±rla
    X, src_name = prepare_features(freq, horizon_td, args.geoid)
    print(f"[OK] Ã–zellikler hazÄ±r (kaynak={src_name}) â€” satÄ±r={len(X):,}, GEOID={X['GEOID'].nunique():,}")

    # Modeli yÃ¼kle
    mdl = load_model(freq)
    print(f"[OK] Model yÃ¼klendi: {MODEL_PATHS[freq].name}")

    # Skorla ve sÄ±rala
    out = score_and_rank(mdl, X, args.topk, args.geoid)

    # Kaydet
    csv_p, json_p = save_outputs(out, freq, args.outdir)
    print(f"[DONE] Kaydedildi â†’ {csv_p}  &  {json_p}")

    # Ã–zet
    by_tier = out["tier"].value_counts().to_dict()
    print("[SUMMARY] tier daÄŸÄ±lÄ±mÄ±:", by_tier)
    print(out.head(min(10, len(out))).to_string(index=False))


if __name__ == "__main__":
    main()
